# Benchmarking Guide

This document explains how to run benchmarks and automatically update README and CHANGELOG with results.

## Quick Reference

```bash
# Run quick mode benchmarks (1-2 minutes)
npm run bench:quick

# Run full benchmarks (5-10 minutes)
npm run bench:all

# Generate detailed report (docs/BENCHMARKS.md)
npm run bench:report

# Update README.md with summary
npm run bench:update-readme

# Update CHANGELOG.md with summary
npm run bench:update-changelog

# Complete workflow (build → bench → report → update README)
npm run bench:workflow

# Or with CHANGELOG update
UPDATE_CHANGELOG=true npm run bench:workflow
```

## Available npm Scripts

### Individual Benchmark Suites

Run a specific benchmark suite:

```bash
npm run bench:transformations      # Map, filter, reduce operations
npm run bench:statistics           # Sum, mean, median, variance, etc.
npm run bench:windowing            # Window, chunk, pairwise operations
npm run bench:terminals            # Terminal operations
npm run bench:lazy-evaluation      # Lazy vs eager evaluation benefits
npm run bench:memory-profiling     # Memory efficiency benchmarks
npm run bench:production-profiling # Real-world production scenarios
```

### Orchestration Scripts

```bash
npm run bench:quick    # Fast mode (1 sec per benchmark) - good for CI/development
npm run bench:all      # Full benchmarks (5 sec per benchmark) - for releases
npm run bench:report   # Generate docs/BENCHMARKS.md detailed report
npm run bench:full     # Shorthand: bench:all && bench:report && bench:update-readme
npm run bench:workflow # Complete: build && bench:all && report && update-readme
```

### Report Generation

```bash
npm run bench:update-readme      # Update README.md with summary table
npm run bench:update-changelog   # Add performance section to CHANGELOG.md
```

## Configuration

### Vitest Benchmark Timeout

Benchmarks timeout is configured in `vitest.config.ts`:

- **Default**: 5 seconds per benchmark
- **Quick mode**: 1 second per benchmark (via `VITEST_BENCH_TIME=1000` env var)
- **Custom**: Set `VITEST_BENCH_TIME=3000` for 3 seconds, etc.

### Memory-Aware Dataset Sizing

Benchmarks automatically scale dataset sizes based on available system memory:

- **Low RAM** (< 2GB): Smaller datasets
- **Medium RAM** (2-8GB): Medium datasets
- **High RAM** (8-32GB): Large datasets
- **Extreme RAM** (> 32GB): Very large datasets

The benchmark suite will skip tests if they would use > 30% of available memory.

## Workflow Examples

### 1. For Development (Quick Iteration)

```bash
# Make changes to the code
npm run bench:quick

# Results saved to benchmarks/results/latest.json
# Check if performance regressed
```

### 2. For Release

```bash
# Before releasing, run complete workflow
npm run bench:workflow

# Optional: Update CHANGELOG too
UPDATE_CHANGELOG=true npm run bench:workflow

# This will:
# 1. Build the project
# 2. Run all benchmarks
# 3. Generate detailed report (docs/BENCHMARKS.md)
# 4. Update README.md with summary
# 5. Optionally update CHANGELOG.md
```

### 3. For CI/CD

```bash
# In CI pipeline, use quick mode for faster feedback
npm run bench:quick

# Or run specific suites that matter for your CI
npm run bench:transformations
npm run bench:statistics
```

## Output Files

### Results

- `benchmarks/results/latest.json` - Latest benchmark results (JSON format)
- `benchmarks/results/v{version}.json` - Versioned results (created once per version)
- `benchmarks/results/raw/` - Raw Vitest output for each run

### Reports

- `docs/BENCHMARKS.md` - Comprehensive benchmark report (generated by `npm run bench:report`)
- `README.md` - Updated with performance summary (generated by `npm run bench:update-readme`)
- `CHANGELOG.md` - Updated with performance notes (generated by `npm run bench:update-changelog`)

## Understanding Results

### Metrics

Each benchmark reports:

- **hz**: Operations per second (higher = faster)
- **min**: Minimum execution time (milliseconds)
- **max**: Maximum execution time (milliseconds)
- **mean**: Average execution time (milliseconds)
- **p75**: 75th percentile latency
- **p99**: 99th percentile latency
- **rme**: Relative margin of error (±%)
- **samples**: Number of samples collected

### Interpretation

When comparing iterflow to native arrays:

- **On small datasets (< 100 items)**: Native arrays are typically faster due to less overhead
- **On large datasets (> 1000 items)**: iterflow can be competitive due to lazy evaluation
- **With early termination** (`.take()`, `.first()`): iterflow often wins significantly
- **With memory-intensive operations** (`.window()`, `.chunk()`): iterflow is more memory-efficient

## Troubleshooting

### Benchmarks timeout

If benchmarks timeout:

```bash
# Check your system memory
free -h  # or on macOS: vm_stat

# Use quick mode
npm run bench:quick

# Or increase timeout
VITEST_BENCH_TIME=10000 npm run bench:all
```

### Memory issues

If you get "out of memory" errors:

1. Close other applications to free memory
2. The benchmark suite will automatically scale down dataset sizes
3. Or manually use quick mode

### Report not generating

If `npm run bench:report` fails:

1. Ensure benchmarks have been run: `npm run bench:all`
2. Check that `benchmarks/results/latest.json` exists
3. Verify you have write permission to `docs/` directory

### README not updating

If `npm run bench:update-readme` fails:

1. Check that `<!-- BENCHMARK_SUMMARY_START -->` markers exist in README.md
2. Ensure `benchmarks/results/latest.json` exists
3. Check file permissions

## Integration with Release Process

Add to your release checklist:

```bash
# Before releasing v0.5.0:
UPDATE_CHANGELOG=true npm run bench:workflow

# This generates:
# - docs/BENCHMARKS.md with detailed results
# - Updates README.md with latest performance summary
# - Updates CHANGELOG.md with performance notes

# Then commit:
git add docs/BENCHMARKS.md README.md CHANGELOG.md benchmarks/results/
git commit -m "perf: update benchmarks for v0.5.0"
git push
```

## Performance Optimization Tips

Based on benchmark results, optimize your usage:

### Use iterflow when:

1. **Processing large datasets** (1000+ items)
2. **Early termination** - using `.take()`, `.first()`, `find()`
3. **Memory is constrained** - lazy evaluation is more efficient
4. **Chaining multiple operations** - benefits from lazy evaluation

### Use native arrays when:

1. **Small datasets** (< 100 items)
2. **Need multiple iterations** - arrays are faster for repeated access
3. **Already materialized** - no cost of wrapping

### Combine both:

```typescript
// Best of both worlds
const result = iter(largeArray)
  .filter(expensive)
  .take(10)           // Early termination = iterflow wins!
  .toArray();
```

## Contributing Benchmarks

To add new benchmarks:

1. Create a new `.bench.ts` file in `benchmarks/`
2. Use `describe()` and `bench()` from Vitest
3. Compare against native arrays, lodash, or ramda
4. Consider memory requirements

Example:

```typescript
import { bench, describe } from 'vitest';
import { iter } from '../src/index.js';

describe('My Benchmark', () => {
  bench('iterflow approach', () => {
    iter(data).filter(x => x > 5).toArray();
  });

  bench('native array approach', () => {
    data.filter(x => x > 5);
  });
});
```

Run `npm run bench:all` to include your new benchmark automatically.
